{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='../do') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, I will show how I solved a not so straigh foward complex web scraping problem. In the process, you will learn the following: \n",
    "- How to do webscraping with Selenium\n",
    "- How to resolve with element identification problems in dynamic pages\n",
    "- How to navigate through pages on a d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "What problem am I trying to solve. I would like to keep track of job postings on this [site](https://careers.un.org/lbw/Home.aspx). Why can't I use regular webscraping packages such as requests or scrapy. The reason is that the website uses dynamic web pages which displays different content each time it's viewed. For example, the page may change with the time of day, the user that accesses the webpage, or the type of user interaction. For instance, in order to view the job postings, you have to submit a search and in order to view all search results, you have to click through the search button. All these things are difficult to achieve with a library requests. Instead, I use [Selenium](https://selenium-python.readthedocs.io) for Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "In order to do able to scrape data from dynamic pages, we need to use **Selenium**, which is the most popular browser automation tool. The best way to use Selenium is via **WebDriver**, a powerful API that builds on top of Selenium and makes calls to a browser to automate it, carrying out actions such as *open this web page*, *click this link*, *see whether the link opens this URL* which are some of the things we will do, etc.\n",
    "\n",
    "In Python, we need to install the package selenium and a supported WebDriverfor browser we want to use. In my case, I installed a Webdriver for Firefox, but you can work with your favorite web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Webdrivers\n",
    "1. Download the latest [GeckoDriver](https://github.com/mozilla/geckodriver/releases/) (for Firefox) and [ChromeDriver](http://chromedriver.storage.googleapis.com/index.html) drivers.\n",
    "2. Unpack them into somewhere fairly easy to navigate to, like the root of your home user directory.\n",
    "3. Add the geckodriver and chromedriver driver's location to your system PATH. For instance, in Mac OS or Linux system, assuming that the path ```~/.local/bin```, then you can place the driver's executable in this folder or modify ```.bash_profile (or .bashrc)``` to include link to where you unpackaded the driver.\n",
    "\n",
    "Please refer to [Mozilla documentation](https://developer.mozilla.org/en-US/docs/Learn/Tools_and_testing/Cross_browser_testing/Your_own_automation_environment) for detailed instructions on how to do this for Windows systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing selenium\n",
    "The Python package can be installed using pip Next, you install the selenium package, using pip or any other package manager you like. And of course you can install within your virtual or global environment.\n",
    "```pip install selenium```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test selenium\n",
    "To test that everything is working, we will do a Google search using selenium instead of using the UI. Its actually fun when you do this for the first time. When using selenium, you can select a ```headless``` option which means you dont see the browser when its opened. Alternatively, you can turn off this option and you can see the broweser as you interact with it using selenium. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of search results About 41,300,000 results (0.81 seconds) \n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.expected_conditions import presence_of_element_located\n",
    "\n",
    "\n",
    "opts = Options()\n",
    "opts.headless = False # turn on or off headless option\n",
    "search_term = 'Lake Malawi'\n",
    "browser = Firefox(options=opts)\n",
    "browser.get(\"https://google.com/\") # open the webpage\n",
    "browser.find_element(By.NAME, \"q\").send_keys(search_term + Keys.RETURN)\n",
    "browser.implicitly_wait(10)\n",
    "results_stats = browser.find_element_by_xpath('//*[@id=\"result-stats\"]')\n",
    "print(\"Number of search results {}\".format(results_stats.text))\n",
    "browser.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other useful tools\n",
    "Just like in any other web scrapign task, its important to be able to accurately identify elements within a webpage's DOM using Xpath and CSS selectors. For this, using built in developers provided in the browsers is useful. In addition, I used [ChroPath](https://chrome.google.com/webstore/detail/chropath/ljngjbnaijcbncmcnjfhigebomdlkcjo?hl=en-US): its an extension for Chrome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'About 38,200,000 results (0.80 seconds) '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_stats.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding the Task at Hand\n",
    "The goal here is to be able to download job posts everyday from this site and dave them to disk as a text file. In order to do this, there are two hoops we have to go through as follows:\n",
    "- Search job openings using the drop down option provided.\n",
    "- Extract all job posts from the search results. While here, we will have to click through all pages of search results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually exploring the site\n",
    "As a requirement, we need to understand the website we would like to get data from, so, we will navigate to the \n",
    "[UN jobs website](https://careers.un.org/lbw/Home.aspx). One of the key things to note is that we can leave as is the default search options and get the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search with Selenium\n",
    "As we saw in the previous section, we can just accept the default search options and proceed to search. \n",
    "In order to do this with selenium, we need to identifify the search button. \n",
    "Once again, you can use developer tools in your favorite browswer to get this information. If you are using chroPath with Google Chrome, it is straightfoward to point on the search button and then copy the xPath for the button. The process is like so:\n",
    "1. Open up developer tools by going to *more tools->developer tools*\n",
    "2. Click on the inspeact element button and then select the search button\n",
    "3. Right click and select copy Xpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = Options()\n",
    "opts.headless = True # turn on or off headless option\n",
    "browser = Firefox(options=opts)\n",
    "browser.get(\"https://careers.un.org/lbw/home.aspx?lang=en-US\")\n",
    "search_button_id = \"ctl00_ContentPlaceHolder1_UNCareersLoader1_ctl00_SearchControl1_btnSearch\"\n",
    "# search with default options\n",
    "browser.find_element(By.ID,search_button_id).send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the Search Results from the Page\n",
    "The next thing we need to do is retrieve the results from the page (i.e, being able to grab the actual job posts). Again using the developer tools, \n",
    "we poke around the search results to find a way to extract the results. There are definately more than one way of doing this, but this is how I achieved it. By checking the XPath for the displayed job positions, I noted that all of them have ```gvSearchGrid``` in their *id*. I therefore decided to use this fact and use an XPath which searches by id and also check that the id contains the afirementioned string like so:\n",
    "\n",
    "```find_elements_by_xpath('//*[contains(@id,\"gvSearchGrid\")]')```\n",
    "\n",
    "Before settling on this solution, I tried another solution which relied on the fact that the search results are stored in a table. As such, using XPath searching for *tr* also works. I ended up not using the *tr* tag because I just had more problems with it than the other approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "xpath_for_search_results = '//*[contains(@id,\"gvSearchGrid\")]'\n",
    "search_results = browser.find_elements_by_xpath(xpath_for_search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the retrieved results\n",
    "For each job post, I'm interested in the following data: position title, grade, duty station and posting period. From the ```find_elements_by_xpath``` function, we a get a list of ```selenium.webdriver.firefox.webelement.FirefoxWebElement```. One of the disadvantage of the XPath string we used is that it gives us elements which arent job posts. As such, we have to sift through them and keep only the relevant items. We can get the text of this a *FirefoxWebElementelement* by invoking the ```text``` method. Thus, we can get the position title as displayed in the search results. Also, we can get all the details of the position using this approach. However, I found it difficult to parse the text from the ```text``` method when it contains all details because of conflicting separators. For instance, for the position shown below, its not easy to use either comma or space as a separator to get the information we need.\n",
    "\n",
    "```DIRECTOR, ECONOMIC AFFAIRS D-2 143143 Economic, Social and Development Programme Management United Nations Conference on Trade and Development GENEVA 15/02/2021```\n",
    "\n",
    "Since each job post shown in the search results is a link to the detailed information about the job, I decided to get the required data by following this link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following clickable links\n",
    "Unlike in other webpages where you can retrieve the url of a link using the \"href\" tag, \n",
    "for javascript enabled pages, the \"href\" returns a java script which runs when you click the link. For instance, lets for one of the job posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL-->javascript:__doPostBack('ctl00$ContentPlaceHolder1$UNCareersLoader1$ctl00$RadtabStrip_Grid1$gvSearchGrid$ctl02$lnkTitle','')\n"
     ]
    }
   ],
   "source": [
    "for i in results_table:\n",
    "    if i.text == 'DIRECTOR, ECONOMIC AFFAIRS':\n",
    "        url = i.get_attribute(\"href\")\n",
    "        print(\"URL--> {}\".format(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we can easily follow the link by using the ```click()``` function. Once we click the page, driver adds another window, we can switch to that window and retrieve the data we need. I do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "for i in search_results:\n",
    "    if i.text == 'DIRECTOR, ECONOMIC AFFAIRS':\n",
    "        i.click()\n",
    "        windows = browser.window_handles\n",
    "        browser.switch_to.window(windows[-1])\n",
    "        pos_title = browser.find_element_by_xpath('//*[contains(@id,\"jobPostingTitle\")]').text\n",
    "        grade = pos_title.split(\",\")[-1]\n",
    "        job_code = browser.find_element_by_xpath('//*[contains(@id,\"jobCodeTitle\")]').text\n",
    "        duty_station = browser.find_element_by_xpath('//*[contains(@id,\"jobDutystation\")]').text\n",
    "        period = browser.find_element_by_xpath('//*[contains(@id,\"jobPeriod\")]').text\n",
    "        url = browser.current_url\n",
    "        pos_html = {'Title': job_code, 'Grade': grade, 'DutyStation': duty_station,\n",
    "                        'PostingPeriod': period, 'url': url}\n",
    "        print(pos_html)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Search Results from All Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more than 10 pages of results and \n",
    "our goal is to scrape all job posts from these pages. There are several minor problems to solve:\n",
    "1. Determine how many pages of results are there\n",
    "2. Progressively access each batch of results (visible on page)\n",
    "3. Follow each page of results. We already know that we have to click through the links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get number of pages\n",
    "The site displays 1-10 pages at a time and a link to click through to the next chunk of 10 pages. To find the total number of pages, we have to click through to the end using ```>>```. The final chunk of 10 pages will have ```<<``` and we use this fact to retrieve the final numbef of pages. In the function ```get_number_of_pages()```, I do the following to get to number of pages.\n",
    "1. On the first page of results, retrieve ```>>``` and use ```click()``` to get to the next chunk of results.\n",
    "2. Use the *tr* tag to get table rows which include the row for page numbers\n",
    "3. Loop through the rows and if we find ```<<```, we know we have hit the last page of results\n",
    "4. Simply extract the last page number from the row of page numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_number_of_pages(guessed_num_pages=40):\n",
    "    \"\"\"\n",
    "    Determine number of pages on the site.\n",
    "    :return: Number of pages\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    with webdriver.Firefox() as driver:\n",
    "        driver.get(\"https://careers.un.org/lbw/home.aspx?lang=en-US\")\n",
    "        driver.find_element(By.ID,\n",
    "                            \"ctl00_ContentPlaceHolder1_UNCareersLoader1_ctl00_SearchControl1_btnSearch\").send_keys(\n",
    "            Keys.RETURN)\n",
    "\n",
    "        driver.implicitly_wait(10)\n",
    "        next_page = driver.find_element_by_link_text(\">>\")\n",
    "        next_page.click()\n",
    "        tr_rows = driver.find_elements_by_tag_name(\"tr\")\n",
    "        try:\n",
    "            for tr in tr_rows:\n",
    "                if \"<<\" in tr.text:\n",
    "                    txt = tr.text.split(\" \")[-1]\n",
    "                    driver.close()\n",
    "                    return int(txt)\n",
    "        except:\n",
    "            # if above fails, just return a large number\n",
    "            driver.close()\n",
    "            return guessed_num_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting Everything Together\n",
    "Lets wrap all the logic in the following three functions.\n",
    "- **search_with_selenium()**. The role of this function is to loop through al pages and within the loop call the rest of the functions.\n",
    "- **get_search_results_from_page()**. For each page, retrieve job posts from that page using the appropriate XPath. The important problem I had to deal with in this function is the fact that only 10 pages are visible at a time. As such, to navigate to page 11, we need to use ```...``` instead of the page number. So, this also happens for page 21  and all odd numbered pages following multiple of 10.\n",
    "- **extract_job_posts()**. Within this function, I sift through the retrieved results as shown earlier and identify which ones are job posts. For each job post, follow the link and extract job post details. One issue I had to deal with here is switching between the main window and the job details window: when you click a job post link, it opens another window and in orde to grab the details, you have to switch to that window. And this required that I switch back to the main window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def search_with_selenium(url=\"https://careers.un.org/lbw/home.aspx?lang=en-US\"):\n",
    "    \"\"\"\n",
    "    Takes the base url and searches for all professional jobs\n",
    "    :return: A list of dict items which have\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    num_pages = get_number_of_pages()\n",
    "    print(\"Found {} pages\".format(num_pages))\n",
    "    # for identifying search button\n",
    "    search_button_id = \"ctl00_ContentPlaceHolder1_UNCareersLoader1_ctl00_SearchControl1_btnSearch\"\n",
    "    with webdriver.Firefox() as driver:\n",
    "        driver.get(url)\n",
    "        driver.find_element(By.ID,search_button_id).send_keys(\n",
    "            Keys.RETURN)\n",
    "\n",
    "        data = []\n",
    "        for page in range(1, num_pages + 1):\n",
    "            try:\n",
    "                results_table = get_search_results_from_page(driver, page)\n",
    "                data += extract_job_posts(driver, results_table)\n",
    "            except:\n",
    "                pass\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_search_results_from_page(driver, page):\n",
    "    \"\"\"\n",
    "    Helper function to extract search results matching job posts.\n",
    "    :return: A list of selenium.webdriver.firefox.webelement.FirefoxWebElement\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    xpath_for_search_results = '//*[contains(@id,\"gvSearchGrid\")]'\n",
    "    if page > 1:\n",
    "        driver.implicitly_wait(10)\n",
    "        next_page = None\n",
    "        # Temporarily hard coded these values as I dont expect to have more than 30 pages\n",
    "        # This could be dynamically generated from the number of pages and visible pages shown\n",
    "        if page == 11:\n",
    "            next_page = driver.find_element_by_link_text(\"...\")\n",
    "        elif page == 21:\n",
    "            # this is because when we hit page 21, we have two ... for going foward and backwards\n",
    "            elements = driver.find_elements_by_link_text(\"...\")\n",
    "            for i in elements:\n",
    "                if str(page) in i.get_attribute(\"href\"):\n",
    "                    next_page = i\n",
    "                    break\n",
    "        else:\n",
    "            next_page = driver.find_element_by_link_text(str(page))\n",
    "        next_page.click()\n",
    "        results_table = driver.find_elements_by_xpath(xpath_for_search_results)\n",
    "    else:\n",
    "        driver.implicitly_wait(10)\n",
    "        results_table = driver.find_elements_by_xpath(xpath_for_search_results)\n",
    "\n",
    "    return results_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def extract_job_posts(driver, results_table):\n",
    "    \"\"\"\n",
    "    Given results on a single page, extract the job details\n",
    "    :param driver:\n",
    "    :type driver:\n",
    "    :param results_table:\n",
    "    :type results_table:\n",
    "    :return:\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    for i in results_table:\n",
    "        try:\n",
    "            if not i.text:\n",
    "                continue\n",
    "            i.click()\n",
    "            windows = driver.window_handles\n",
    "            if len(windows) == 1:\n",
    "                continue\n",
    "            driver.switch_to.window(windows[-1])\n",
    "            try:\n",
    "                pos_title = driver.find_element_by_xpath('//*[contains(@id,\"jobPostingTitle\")]').text\n",
    "                grade = pos_title.split(\",\")[-1]\n",
    "            except:\n",
    "                grade = 'Not specified'\n",
    "\n",
    "            try:\n",
    "                job_code = driver.find_element_by_xpath('//*[contains(@id,\"jobCodeTitle\")]').text\n",
    "            except:\n",
    "                job_code = i.text\n",
    "\n",
    "            duty_station = driver.find_element_by_xpath('//*[contains(@id,\"jobDutystation\")]').text\n",
    "            period = driver.find_element_by_xpath('//*[contains(@id,\"jobPeriod\")]').text\n",
    "            url = driver.current_url\n",
    "            pos_html = {'Title': job_code, 'Grade': grade, 'DutyStation': duty_station,\n",
    "                        'PostingPeriod': period, 'url': url}\n",
    "            data.append(pos_html)\n",
    "            driver.close()\n",
    "            driver.switch_to.window(windows[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            driver.switch_to.window(windows[0])\n",
    "            continue\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concluding Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I have showed how I used selenium to scrape data off of \n",
    "a website where content is dynamically loaded using  javascript. So, one big question is why not use *requests*, *Beatifulsoup* or *scrapy*? For web pages where data is dynamically loaded,  the data is only loaded when you click on a link or perfom some other action in the browser, as such, you cannot reach the desired data by downloading it using **requests** or standard **scrapy** code. Although scrapy do have advanced features to deal with this issue, for my simple use case, I found that using **selenium** was much easier. Its worth mentinoning that selenium can be used in combination with scrapy as explained [here](https://stackoverflow.com/questions/17975471/selenium-with-scrapy-for-dynamic-page).\n",
    "\n",
    "The task that I perfom in this notebook is straighfoward. However, navigating through pages to ensure that I scraped data from all pages was slightly tricky and I have shown how I resolved the problem. Also, with the code I have, I still have this error coming up: *Web element reference not seen before* which means the code could use some improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
